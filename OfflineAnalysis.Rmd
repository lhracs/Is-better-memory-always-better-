---
title: "OfflineAnalysis"
author: "Mahyar Nakhaei"
date: "10/05/2020"
output: rmarkdown::github_document

---

##**Introduction**  

This study investigates how the memory load on speakers, ambiguity of the contexts and their working memory capacity (WMC) influence their referent choice for English pronouns. To do so, stimuli contain 7 regions which are read in a self-paced reading method. Some stimuli are preceded by an image showing geometric shapes. After reading the whole sentence, participants are required to choose the best referent choice (subject antecedent, object antecedent or neither) as in (1) below.  
  
(1) a. **Tom** / said hello / to / Mary / while / **he** / was crossing the street.  
    b. **Tom** / said hello / to / **Mark** / while / **he** / was crossing the street.  
    c. Tom / said hello / to / Mark / while / **?she** / was crossing the street. 
    
    Question: Who was crossing the street?    
    1- Tom  
    2- Mark/Mary  
    3- Neither
  
The codes below explain how I ran multinomial logistic regression on dataset with Ambiguity, MemoryLoad and WMC of participants as independent variables and ReferentChoice as dependent variable.  
  
```{r}
library(nnet)
```
This is the required library for multinom logistic regression. 
  
##**Recalling dataset and filtering data for only experimental stimul and outliers:**  
  
```{r}
dataset<-read.table("MasterDataSet.txt",header=T)
Exp<-subset(dataset,ItemType=="Exp")
ExpFiltered<-subset(Exp,TotalRT<(mean(TotalRT)+2.5*sd(TotalRT))&TotalRT>(mean(TotalRT)-2.5*sd(TotalRT)))
```
This last line filters out outliers
    
##**Applying the most complex multinomial log regression:**  
  
```{r}
test <- multinom(ReferentChoice ~ Ambiguity + MemoryLoad + WMC, data = ExpFiltered)
z <- summary(test)$coefficients/summary(test)$standard.errors
p <- (1 - pnorm(abs(z), 0, 1))*2
p
summary(test)
```
    
These are the stages that we should go through for any multinom analysis. From now on, we try to find out the best-fitting model!  
First, we should include everything except unwanted columns through:  
  
```{r}
data1 <- ExpFiltered[,-c(1,2,3,4,5,8,9,10,11,12,13,14,16,17,18,19,20,21,22,23,24,25)]
Model1 <- multinom(ReferentChoice ~ ., data = data1)
```
Then, we do all stages discussed first. That is:  
```{r}
z <- summary(Model1)$coefficients/summary(Model1)$standard.errors
p <- (1 - pnorm(abs(z), 0, 1))*2
p
summary(Model1)
```
  
Now, we should remove columns which has no significant p-values above (In this case, MemoryLoad)  
  
```{r}
data2 <- ExpFiltered[,-c(1,2,3,4,5,8,9,10,11,12,13,14,16,17,18,19,20,21,22,23,24,25)]
Model2 <- multinom(ReferentChoice ~ .-MemoryLoad, data = data2)
```
  
Now, we should repeat the same stages for this new model:    
```{r}
z <- summary(Model2)$coefficients/summary(Model2)$standard.errors
p <- (1 - pnorm(abs(z), 0, 1))*2
p
summary(Model2)
```
  
##**The interpretations:**  
  
  Looking at the table of summary shown above, we can conclude that:  
  1) **The memory load** on speakers has **no significant effect** on their **referent choice**.  
  2) When the sentence changes from Ambiguous to No Referent, the log odds (likelihood) of choosing **Object** as the referent compared to **neither** decreases by -4.0255 whereas the log odds of choosing **subject** compared to **neither** decreases by -3.8983.  
In other words, when the sentence has **no referent**, participants prefer to select **neither** option significantly more than **subject** which is followed by **object** as stated in (2):  

(2) Tom said hello to Mark while **?she** was crossing the street. 
    
    Question: Who was crossing the street?    
    1- Tom  
    2- Mark  
    3- Neither
    
    **Prediction:** Neither > Tom > Mary  
  
In addition, the summary shows that when the sentence changes from **Ambiguous** to **One Referent**, no significant effect is observed in participants' referent choice. In other words, they are more tolerant of ambiguous sentences compared to no referent sentences.  
  
Finally and surprisingly, a one-unit increase in WMC scores almost equally decreases the log odds of selecting subject or object antecedents compared to neither which means that participants with higher working memory prefer to select neither options signicantly more!!!  

```{r}

## Determine goodness of fit for models using Hosmer-Lemeshow test

library("generalhoslem")
library("mlogit")

HL1 <- logitgof(data1$ReferentChoice, fitted(Model1))
HL2 <- logitgof(data2$ReferentChoice, fitted(Model2))

HL1
HL2
```




